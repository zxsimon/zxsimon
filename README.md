## Hello!
I am a machine learning enthusiast interested in unlocking the potential of foundation models through post-training. I'm currently most intrigued by the following problems:
1. Small Models: How can we effectively leverage smaller models—through parameter-efficient fine-tuning and other methods—to solve tasks accurately and efficiently, without the compute and memory overhead that large models demand?
2. Latent Reasoning: What is the precise mechanism through which chain-of-thought reasoning elicits better answers from LLMs? To what extent do these underlying pathways resemble human reasoning? How can we achieve cleaner, more efficient, model-native reasoning processes that overcome: (1) the autoregressive nature of token-by-token generation, and (2) the inherent information loss from recursive token generation's dimensionality reduction?
3. Data-Efficient Post-Training: Data is plentiful, high-quality data is scarce, and training-grade data is scarcer still. How can we effectively conduct post-training and fine-tuning in data-constrained environments? Some ideas include: 1) creating a GRPO-like RL algorithm that allows the model to "reflect" and update its weights repeatedly on a single row of data (i.e., one example -> multiple rollouts and episodes), perhaps via an entropy-aware, tree-like exploration scheme, and 2) using an evolutionary algorithm to synthetically create high-quality data based on few-shot examples, to scale up small datasets.
