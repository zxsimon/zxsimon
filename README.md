## Hello!
I am a machine learning enthusiast interested in unlocking the potential of foundation models through post-training. I'm currently most intrigued by the following problems:
1. Small Models: How can we effectively leverage smaller models—through parameter-efficient fine-tuning and other methods—to solve tasks accurately and efficiently, without the compute and memory overhead that large models demand?
2. Latent Reasoning: What is the precise mechanism through which chain-of-thought reasoning elicits better answers from LLMs? To what extent do these underlying pathways resemble human reasoning? How can we achieve cleaner, more efficient, model-native reasoning processes that overcome: (1) the autoregressive nature of token-by-token generation, and (2) the inherent information loss from recursive token generation's dimensionality reduction?
3. Data-Efficient Supervised Fine-Tuning: Data is plentiful, high-quality data is scarce, and training-grade data is scarcer still. How can we effectively conduct SFT in data-constrained environments—perhaps drawing inspiration from parallel rollouts in RL—by repeatedly updating model weights on limited datasets or their minimal variations?
